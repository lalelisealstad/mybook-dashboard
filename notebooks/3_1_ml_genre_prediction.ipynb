{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting book genre from book description \n",
    "## Multi-class classification\n",
    "\n",
    "highly used resource: https://www.kaggle.com/code/prathameshgadekar/book-genre-prediction-nlp/notebook\n",
    "\n",
    "Selecting MUltinomialNB since their analysis evaluated that to be the best model for that dataset, and since I am using the same dataframe, as I have not been able to find other dataframes with both book description and genre, I find it appropriate.\n",
    "\n",
    "Resources: \n",
    "    - dataset: https://github.com/uchidalab/book-dataset/tree/master and collect description from google books api\n",
    "    - dataset:  https://www.kaggle.com/datasets/athu1105/book-genre-prediction\n",
    "    - source reference: https://www.kaggle.com/code/majinx/nlp-book-genre-prediction-eda-and-modelling#Testing-Different-Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.27.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import missingno as msno #For missing value visualization\n",
    "\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For NLP\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "# from wordcloud import WordCloud\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Modelling Purpose\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB,BernoulliNB,MultinomialNB\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/elisealstad/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../assets/data.csv')\n",
    "amazondf = pd.read_pickle('../assets/amazon_books_description.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('index',inplace = True,axis = 1)\n",
    "data = data.rename(columns={'title':'Title', 'summary':'Description' })\n",
    "new_labels_data =  {'fantasy':\"Science Fiction & Fantasy\" ,\n",
    "                     'psychology': 'psychology and self-help'}\n",
    "data['genre'] = data['genre'].replace(new_labels_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Author(s)</th>\n",
       "      <th>Publish_Date</th>\n",
       "      <th>Description</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Page_Count</th>\n",
       "      <th>Categories</th>\n",
       "      <th>Average_Rating</th>\n",
       "      <th>Rating_Count</th>\n",
       "      <th>Language</th>\n",
       "      <th>genre</th>\n",
       "      <th>Title_org</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>That's That</td>\n",
       "      <td>Colin Broderick</td>\n",
       "      <td>2013-05-07</td>\n",
       "      <td>A brutally honest and deeply affecting memoir ...</td>\n",
       "      <td>9780307716347</td>\n",
       "      <td>368.0</td>\n",
       "      <td>Biography &amp; Autobiography</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Title        Author(s) Publish_Date  \\\n",
       "0  That's That  Colin Broderick   2013-05-07   \n",
       "\n",
       "                                         Description           ISBN  \\\n",
       "0  A brutally honest and deeply affecting memoir ...  9780307716347   \n",
       "\n",
       "   Page_Count                 Categories  Average_Rating  Rating_Count  \\\n",
       "0       368.0  Biography & Autobiography             NaN           NaN   \n",
       "\n",
       "  Language genre Title_org  \n",
       "0       en   NaN       NaN  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazondf.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([data, amazondf])\n",
    "df = (\n",
    "    df.dropna(subset=['Description', 'genre'])\n",
    "    .filter(items=['Title', 'Description','genre'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7716, 3)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning unecessary text from the string \n",
    "Stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def clean(text):\n",
    "    text = text.lower() #Converting to lowerCase\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ',text) #removing punctuation\n",
    "    \n",
    "    text_tokens = word_tokenize(text) #removing stopwords\n",
    "    tw = [word for word in text_tokens if not word in Stopwords]\n",
    "    text = (\" \").join(tw)\n",
    "    \n",
    "    splt = text.split(' ')\n",
    "    output = [x for x in splt if len(x) > 3] #removing words with length<=3\n",
    "    text = (\" \").join(output)\n",
    "    \n",
    "    text = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', text) #removing single character \n",
    "    text = re.sub('<.*?>+',' ',text) #removing HTML Tags\n",
    "    text = re.sub('\\n', ' ',text) #removal of new line characters\n",
    "    text = re.sub(r'\\s+', ' ',text) #removal of multiple spaces\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "genre\n",
       "thriller                      1263\n",
       "Science Fiction & Fantasy     1051\n",
       "history                        789\n",
       "science                        647\n",
       "horror                         600\n",
       "crime                          500\n",
       "romance                        325\n",
       "psychology and self-help       306\n",
       "sports                         294\n",
       "travel                         246\n",
       "Biographies & Memoirs          235\n",
       "Business & Money               233\n",
       "Children's Books               217\n",
       "Politics & Social Sciences     215\n",
       "Cookbooks, Food & Wine         208\n",
       "Crafts, Hobbies & Home         198\n",
       "Health                         196\n",
       "Teen & YA                      193\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.genre.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Most common words in Science Fiction & Fantasy column:\n",
      "one: 1088\n",
      "world: 719\n",
      "new: 677\n",
      "king: 585\n",
      "time: 581\n",
      "find: 554\n",
      "two: 552\n",
      "back: 537\n",
      "life: 495\n",
      "also: 492\n",
      "\n",
      "\n",
      " Most common words in science column:\n",
      "one: 691\n",
      "time: 637\n",
      "world: 486\n",
      "earth: 464\n",
      "planet: 425\n",
      "new: 421\n",
      "ship: 403\n",
      "human: 375\n",
      "first: 343\n",
      "life: 337\n",
      "\n",
      "\n",
      " Most common words in crime column:\n",
      "one: 550\n",
      "murder: 450\n",
      "man: 378\n",
      "police: 326\n",
      "also: 314\n",
      "two: 313\n",
      "poirot: 297\n",
      "death: 295\n",
      "house: 280\n",
      "found: 280\n",
      "\n",
      "\n",
      " Most common words in history column:\n",
      "one: 701\n",
      "father: 579\n",
      "war: 492\n",
      "time: 450\n",
      "story: 450\n",
      "also: 449\n",
      "new: 449\n",
      "two: 447\n",
      "book: 428\n",
      "first: 427\n",
      "\n",
      "\n",
      " Most common words in horror column:\n",
      "one: 731\n",
      "anita: 541\n",
      "house: 453\n",
      "new: 404\n",
      "also: 403\n",
      "man: 385\n",
      "back: 383\n",
      "two: 371\n",
      "life: 365\n",
      "find: 347\n",
      "\n",
      "\n",
      " Most common words in thriller column:\n",
      "one: 1172\n",
      "new: 784\n",
      "two: 595\n",
      "life: 556\n",
      "less: 536\n",
      "man: 534\n",
      "alex: 488\n",
      "find: 462\n",
      "time: 454\n",
      "first: 437\n",
      "\n",
      "\n",
      " Most common words in psychology and self-help column:\n",
      "book: 266\n",
      "life: 231\n",
      "new: 176\n",
      "us: 174\n",
      "people: 173\n",
      "one: 144\n",
      "love: 142\n",
      "world: 109\n",
      "make: 99\n",
      "work: 98\n",
      "\n",
      "\n",
      " Most common words in romance column:\n",
      "one: 228\n",
      "love: 225\n",
      "new: 205\n",
      "life: 184\n",
      "less: 118\n",
      "world: 109\n",
      "book: 105\n",
      "man: 100\n",
      "family: 94\n",
      "series: 89\n",
      "\n",
      "\n",
      " Most common words in sports column:\n",
      "one: 170\n",
      "book: 169\n",
      "team: 111\n",
      "new: 111\n",
      "world: 109\n",
      "less: 104\n",
      "game: 99\n",
      "life: 91\n",
      "first: 86\n",
      "time: 75\n",
      "\n",
      "\n",
      " Most common words in travel column:\n",
      "travel: 165\n",
      "world: 138\n",
      "guide: 101\n",
      "new: 95\n",
      "book: 91\n",
      "city: 81\n",
      "life: 78\n",
      "country: 78\n",
      "people: 77\n",
      "one: 76\n",
      "\n",
      "\n",
      " Most common words in Biographies & Memoirs column:\n",
      "life: 181\n",
      "one: 162\n",
      "story: 154\n",
      "new: 119\n",
      "world: 117\n",
      "book: 105\n",
      "history: 91\n",
      "years: 90\n",
      "family: 80\n",
      "man: 77\n",
      "\n",
      "\n",
      " Most common words in Cookbooks, Food & Wine column:\n",
      "recipes: 334\n",
      "food: 167\n",
      "book: 155\n",
      "cooking: 131\n",
      "cookbook: 116\n",
      "new: 111\n",
      "one: 94\n",
      "dishes: 88\n",
      "delicious: 81\n",
      "ingredients: 78\n",
      "\n",
      "\n",
      " Most common words in Children's Books column:\n",
      "book: 118\n",
      "new: 73\n",
      "children: 62\n",
      "readers: 52\n",
      "kids: 49\n",
      "books: 48\n",
      "time: 46\n",
      "young: 41\n",
      "series: 41\n",
      "world: 41\n",
      "\n",
      "\n",
      " Most common words in Teen & YA column:\n",
      "life: 73\n",
      "book: 68\n",
      "new: 65\n",
      "one: 48\n",
      "school: 48\n",
      "world: 46\n",
      "young: 45\n",
      "story: 37\n",
      "history: 34\n",
      "first: 34\n",
      "\n",
      "\n",
      " Most common words in Health column:\n",
      "book: 129\n",
      "life: 95\n",
      "help: 81\n",
      "new: 73\n",
      "health: 70\n",
      "guide: 62\n",
      "diet: 56\n",
      "people: 55\n",
      "treatment: 54\n",
      "well: 49\n",
      "\n",
      "\n",
      " Most common words in Crafts, Hobbies & Home column:\n",
      "book: 125\n",
      "guide: 69\n",
      "new: 64\n",
      "techniques: 53\n",
      "wedding: 47\n",
      "information: 43\n",
      "make: 42\n",
      "including: 40\n",
      "projects: 40\n",
      "art: 40\n",
      "\n",
      "\n",
      " Most common words in Business & Money column:\n",
      "book: 194\n",
      "business: 171\n",
      "new: 160\n",
      "financial: 121\n",
      "management: 86\n",
      "world: 82\n",
      "market: 81\n",
      "edition: 80\n",
      "industry: 71\n",
      "need: 70\n",
      "\n",
      "\n",
      " Most common words in Politics & Social Sciences column:\n",
      "book: 146\n",
      "new: 124\n",
      "one: 83\n",
      "work: 79\n",
      "edition: 73\n",
      "world: 73\n",
      "women: 67\n",
      "philosophy: 66\n",
      "life: 61\n",
      "human: 52\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "def most_common_words(column, n=10):\n",
    "    # Tokenize and lowercase the words\n",
    "    words = word_tokenize(\" \".join(column.str.lower()))\n",
    "\n",
    "    # Remove stopwords (common words like 'the', 'and', etc.)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "\n",
    "    # Calculate word frequencies\n",
    "    word_freq = Counter(words)\n",
    "\n",
    "    # Get the most common words\n",
    "    common_words = word_freq.most_common(n)\n",
    "\n",
    "    return common_words\n",
    "\n",
    "for genre in df.genre.unique().tolist(): \n",
    "    most_common_words_description = most_common_words(df.query('genre==@genre')['Description'], n=10)\n",
    "    print('\\n\\n',f\"Most common words in {genre} column:\")\n",
    "    for word, count in most_common_words_description:\n",
    "        print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing \n",
    "\n",
    "def data_preprocessing(text):\n",
    "    tokens = word_tokenize(text) #Tokenization\n",
    "    tokens = [WordNetLemmatizer().lemmatize(word) for word in tokens] #Lemmetization\n",
    "    tokens = [SnowballStemmer(language = 'english').stem(word) for word in tokens] #Stemming\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Drowned Wednesday</td>\n",
       "      <td>drown wednesday is the first truste among the ...</td>\n",
       "      <td>Science Fiction &amp; Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Lost Hero</td>\n",
       "      <td>as the book open , jason awaken on a school bu...</td>\n",
       "      <td>Science Fiction &amp; Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Eyes of the Overworld</td>\n",
       "      <td>cugel is easili persuad by the merchant fianos...</td>\n",
       "      <td>Science Fiction &amp; Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Magic's Promise</td>\n",
       "      <td>the book open with herald-mag vanyel return to...</td>\n",
       "      <td>Science Fiction &amp; Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Taran Wanderer</td>\n",
       "      <td>taran and gurgi have return to caer dallben fo...</td>\n",
       "      <td>Science Fiction &amp; Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814</th>\n",
       "      <td>Claimed</td>\n",
       "      <td>sometim , the hero must be the villain ... fou...</td>\n",
       "      <td>romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815</th>\n",
       "      <td>Outsmart Your Cancer</td>\n",
       "      <td>book &amp; cd . this easy-to-read altern treatment...</td>\n",
       "      <td>Health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816</th>\n",
       "      <td>Bistronomy</td>\n",
       "      <td>finalist for the iacp cookbook award , chef an...</td>\n",
       "      <td>Cookbooks, Food &amp; Wine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>A Guide to Japanese Hot Springs</td>\n",
       "      <td>this text is a guid to over 160 of the best ho...</td>\n",
       "      <td>travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>The Winner</td>\n",
       "      <td>this book ha been design to help explain asthm...</td>\n",
       "      <td>Health</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7716 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Title  \\\n",
       "0                  Drowned Wednesday   \n",
       "1                      The Lost Hero   \n",
       "2          The Eyes of the Overworld   \n",
       "3                    Magic's Promise   \n",
       "4                     Taran Wanderer   \n",
       "..                               ...   \n",
       "814                          Claimed   \n",
       "815             Outsmart Your Cancer   \n",
       "816                       Bistronomy   \n",
       "817  A Guide to Japanese Hot Springs   \n",
       "818                       The Winner   \n",
       "\n",
       "                                           Description  \\\n",
       "0    drown wednesday is the first truste among the ...   \n",
       "1    as the book open , jason awaken on a school bu...   \n",
       "2    cugel is easili persuad by the merchant fianos...   \n",
       "3    the book open with herald-mag vanyel return to...   \n",
       "4    taran and gurgi have return to caer dallben fo...   \n",
       "..                                                 ...   \n",
       "814  sometim , the hero must be the villain ... fou...   \n",
       "815  book & cd . this easy-to-read altern treatment...   \n",
       "816  finalist for the iacp cookbook award , chef an...   \n",
       "817  this text is a guid to over 160 of the best ho...   \n",
       "818  this book ha been design to help explain asthm...   \n",
       "\n",
       "                         genre  \n",
       "0    Science Fiction & Fantasy  \n",
       "1    Science Fiction & Fantasy  \n",
       "2    Science Fiction & Fantasy  \n",
       "3    Science Fiction & Fantasy  \n",
       "4    Science Fiction & Fantasy  \n",
       "..                         ...  \n",
       "814                    romance  \n",
       "815                     Health  \n",
       "816     Cookbooks, Food & Wine  \n",
       "817                     travel  \n",
       "818                     Health  \n",
       "\n",
       "[7716 rows x 3 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Description'] = df['Description'].apply(data_preprocessing)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       7\n",
       "1       7\n",
       "2       7\n",
       "3       7\n",
       "4       7\n",
       "       ..\n",
       "814    13\n",
       "815     5\n",
       "816     3\n",
       "817    17\n",
       "818     5\n",
       "Name: genre_vec, Length: 7716, dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Converting all the categorical features of 'genre' to numerical\n",
    "\n",
    "labelencoder = LabelEncoder()\n",
    "df['genre_vec'] = labelencoder.fit_transform(df['genre'])\n",
    "df['genre_vec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Science Fiction & Fantasy', 'Science Fiction & Fantasy',\n",
       "       'Science Fiction & Fantasy', ..., 'Cookbooks, Food & Wine',\n",
       "       'travel', 'Health'], dtype=object)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelencoder.inverse_transform(df['genre_vec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(df['Description'])\n",
    "y = df['genre_vec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test different ML models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [BernoulliNB(),MultinomialNB(),SGDClassifier(),LogisticRegression(),RandomForestClassifier(),GradientBoostingClassifier(),\n",
    "         AdaBoostClassifier(),SVC(),DummyClassifier(),ExtraTreeClassifier(),KNeighborsClassifier()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB Successfully Trained\n",
      "MultinomialNB Successfully Trained\n",
      "SGDClassifier Successfully Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elisealstad/Desktop/Code/mybook-dashboard/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression Successfully Trained\n",
      "RandomForestClassifier Successfully Trained\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m Name\u001b[38;5;241m.\u001b[39mappend(name)\n\u001b[1;32m     12\u001b[0m begin \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 13\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m prediction \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m     15\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/Desktop/Code/mybook-dashboard/.venv/lib/python3.12/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Code/mybook-dashboard/.venv/lib/python3.12/site-packages/sklearn/ensemble/_gb.py:532\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resize_state()\n\u001b[1;32m    531\u001b[0m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[0;32m--> 532\u001b[0m n_stages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbegin_at_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_stages \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m~/Desktop/Code/mybook-dashboard/.venv/lib/python3.12/site-packages/sklearn/ensemble/_gb.py:610\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stages\u001b[0;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[1;32m    603\u001b[0m         initial_loss \u001b[38;5;241m=\u001b[39m loss_(\n\u001b[1;32m    604\u001b[0m             y[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[1;32m    605\u001b[0m             raw_predictions[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[1;32m    606\u001b[0m             sample_weight[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[1;32m    607\u001b[0m         )\n\u001b[1;32m    609\u001b[0m \u001b[38;5;66;03m# fit next stage of trees\u001b[39;00m\n\u001b[0;32m--> 610\u001b[0m raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;66;03m# track loss\u001b[39;00m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n",
      "File \u001b[0;32m~/Desktop/Code/mybook-dashboard/.venv/lib/python3.12/site-packages/sklearn/ensemble/_gb.py:245\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stage\u001b[0;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[1;32m    242\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;241m*\u001b[39m sample_mask\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[1;32m    244\u001b[0m X \u001b[38;5;241m=\u001b[39m X_csr \u001b[38;5;28;01mif\u001b[39;00m X_csr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[0;32m--> 245\u001b[0m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# update tree leaves\u001b[39;00m\n\u001b[1;32m    248\u001b[0m loss\u001b[38;5;241m.\u001b[39mupdate_terminal_regions(\n\u001b[1;32m    249\u001b[0m     tree\u001b[38;5;241m.\u001b[39mtree_,\n\u001b[1;32m    250\u001b[0m     X,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    257\u001b[0m     k\u001b[38;5;241m=\u001b[39mk,\n\u001b[1;32m    258\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/Code/mybook-dashboard/.venv/lib/python3.12/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Code/mybook-dashboard/.venv/lib/python3.12/site-packages/sklearn/tree/_classes.py:1320\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m   1290\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m \n\u001b[1;32m   1294\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1317\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m   1318\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1320\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/Code/mybook-dashboard/.venv/lib/python3.12/site-packages/sklearn/tree/_classes.py:443\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    433\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    434\u001b[0m         splitter,\n\u001b[1;32m    435\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    441\u001b[0m     )\n\u001b[0;32m--> 443\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "Name = []\n",
    "Accuracy = []\n",
    "Precision = []\n",
    "F1_Score = []\n",
    "Recall = []\n",
    "Time_Taken = []\n",
    "for model in models:\n",
    "    name = type(model).__name__\n",
    "    Name.append(name)\n",
    "    begin = time.time()\n",
    "    model.fit(X_train,y_train)\n",
    "    prediction = model.predict(X_test)\n",
    "    end = time.time()\n",
    "    Accuracy.append(accuracy_score(prediction,y_test))\n",
    "    Precision.append(precision_score(prediction,y_test,average = 'macro'))\n",
    "    Recall.append(recall_score(prediction, y_test, average='macro', zero_division=1))\n",
    "    F1_Score.append(f1_score(prediction,y_test,average = 'macro'))\n",
    "    Time_Taken.append(end-begin)\n",
    "    print(name + ' Successfully Trained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision_score</th>\n",
       "      <th>Recall_score</th>\n",
       "      <th>F1_score</th>\n",
       "      <th>Time Taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.330935</td>\n",
       "      <td>0.137544</td>\n",
       "      <td>0.137544</td>\n",
       "      <td>0.133584</td>\n",
       "      <td>0.052388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.534532</td>\n",
       "      <td>0.339410</td>\n",
       "      <td>0.339410</td>\n",
       "      <td>0.351079</td>\n",
       "      <td>0.022617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SGDClassifier</td>\n",
       "      <td>0.599281</td>\n",
       "      <td>0.535043</td>\n",
       "      <td>0.535043</td>\n",
       "      <td>0.538543</td>\n",
       "      <td>0.338006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.605036</td>\n",
       "      <td>0.551865</td>\n",
       "      <td>0.551865</td>\n",
       "      <td>0.557967</td>\n",
       "      <td>6.776117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.477698</td>\n",
       "      <td>0.359631</td>\n",
       "      <td>0.359631</td>\n",
       "      <td>0.380613</td>\n",
       "      <td>17.339131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>0.530935</td>\n",
       "      <td>0.429237</td>\n",
       "      <td>0.429237</td>\n",
       "      <td>0.466317</td>\n",
       "      <td>351.766833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>0.298561</td>\n",
       "      <td>0.235048</td>\n",
       "      <td>0.235048</td>\n",
       "      <td>0.247421</td>\n",
       "      <td>9.825970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.360432</td>\n",
       "      <td>0.193885</td>\n",
       "      <td>0.193885</td>\n",
       "      <td>0.197525</td>\n",
       "      <td>35.842732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DummyClassifier</td>\n",
       "      <td>0.174101</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.016476</td>\n",
       "      <td>0.000865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ExtraTreeClassifier</td>\n",
       "      <td>0.213669</td>\n",
       "      <td>0.186854</td>\n",
       "      <td>0.186854</td>\n",
       "      <td>0.183106</td>\n",
       "      <td>0.320879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>0.215108</td>\n",
       "      <td>0.177698</td>\n",
       "      <td>0.177698</td>\n",
       "      <td>0.172192</td>\n",
       "      <td>0.430551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Name  Accuracy  Precision_score  Recall_score  \\\n",
       "0                  BernoulliNB  0.330935         0.137544      0.137544   \n",
       "1                MultinomialNB  0.534532         0.339410      0.339410   \n",
       "2                SGDClassifier  0.599281         0.535043      0.535043   \n",
       "3           LogisticRegression  0.605036         0.551865      0.551865   \n",
       "4       RandomForestClassifier  0.477698         0.359631      0.359631   \n",
       "5   GradientBoostingClassifier  0.530935         0.429237      0.429237   \n",
       "6           AdaBoostClassifier  0.298561         0.235048      0.235048   \n",
       "7                          SVC  0.360432         0.193885      0.193885   \n",
       "8              DummyClassifier  0.174101         0.055556      0.055556   \n",
       "9          ExtraTreeClassifier  0.213669         0.186854      0.186854   \n",
       "10        KNeighborsClassifier  0.215108         0.177698      0.177698   \n",
       "\n",
       "    F1_score  Time Taken  \n",
       "0   0.133584    0.052388  \n",
       "1   0.351079    0.022617  \n",
       "2   0.538543    0.338006  \n",
       "3   0.557967    6.776117  \n",
       "4   0.380613   17.339131  \n",
       "5   0.466317  351.766833  \n",
       "6   0.247421    9.825970  \n",
       "7   0.197525   35.842732  \n",
       "8   0.016476    0.000865  \n",
       "9   0.183106    0.320879  \n",
       "10  0.172192    0.430551  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dict = {'Name':Name,'Accuracy':Accuracy,'Precision_score':Precision,'Recall_score':Precision,\n",
    "        'F1_score':F1_Score,'Time Taken':Time_Taken}\n",
    "model_df = pd.DataFrame(Dict)\n",
    "model_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run and store best model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best fitting model \n",
    "model =LogisticRegression(max_iter=1000)\n",
    "model.fit(X,y)\n",
    "prediction = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename = \"model.pickle\"\n",
    "\n",
    "# save model\n",
    "pickle.dump(model, open(filename, \"wb\"))\n",
    "\n",
    "# load model\n",
    "loaded_model = pickle.load(open(filename, \"rb\"))\n",
    "\n",
    "# you can use loaded model to compute predictions\n",
    "y_predicted = loaded_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "mybooks = pd.read_pickle('../assets/my_books.pkl')\n",
    "mybooks = mybooks.query('Description.notna()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      japanes fairi tale - enchant , enigmat stori o...\n",
       "1      ' a sensual feast of a novel , written with el...\n",
       "2      a new york time , usa today , and washington p...\n",
       "3      * the sunday time number one bestsel * * over ...\n",
       "4      the addict no.1 bestsel that everyon is talk a...\n",
       "                             ...                        \n",
       "359    the key to rebecca is a grip thriller set dure...\n",
       "360    winner of the pulitz prize , a new york time b...\n",
       "361    one of the most influenti book of the twentiet...\n",
       "362    in this deepli stir novel , acclaim author cri...\n",
       "363    mr jone of manor farm is so lazi and drunken t...\n",
       "Name: Description, Length: 332, dtype: object"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mybooks['Description_cleaned'] = mybooks['Description'].apply(clean)\n",
    "\n",
    "# data preprocessing \n",
    "mybooks['Description'] = mybooks['Description'].apply(data_preprocessing)\n",
    "mybooks['Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting all the categorical features of 'genre' to numerical\n",
    "nyX = cv.transform(mybooks['Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can use loaded model to compute predictions\n",
    "genre = loaded_model.predict(nyX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Science Fiction & Fantasy' 'travel' 'Science Fiction & Fantasy'\n",
      " 'psychology and self-help' 'romance' 'travel' 'thriller' 'thriller'\n",
      " 'thriller' 'thriller' 'thriller' 'thriller' 'thriller'\n",
      " 'Science Fiction & Fantasy' \"Children's Books\" 'romance' 'romance'\n",
      " 'thriller' 'Teen & YA' 'thriller' 'thriller' 'thriller' 'thriller'\n",
      " 'thriller' 'thriller' 'thriller' 'thriller' 'thriller' 'Teen & YA'\n",
      " 'romance' 'romance' 'Science Fiction & Fantasy'\n",
      " 'Science Fiction & Fantasy' 'psychology and self-help' 'thriller'\n",
      " 'thriller' 'romance' 'romance' 'Science Fiction & Fantasy' 'romance'\n",
      " 'horror' 'Cookbooks, Food & Wine' 'thriller' 'history' 'thriller'\n",
      " 'romance' 'Science Fiction & Fantasy' 'thriller' 'Biographies & Memoirs'\n",
      " 'Science Fiction & Fantasy' 'travel' 'thriller' 'thriller' 'crime'\n",
      " 'thriller' 'Business & Money' 'thriller' 'thriller' 'romance' 'thriller'\n",
      " 'thriller' 'thriller' 'thriller' 'thriller' 'psychology and self-help'\n",
      " 'Teen & YA' 'romance' 'psychology and self-help' 'Business & Money'\n",
      " 'thriller' 'romance' 'romance' 'thriller' 'thriller' 'travel' 'romance'\n",
      " 'thriller' 'thriller' 'history' 'thriller' 'sports' 'Business & Money'\n",
      " 'romance' 'horror' 'science' 'sports' 'romance' 'history'\n",
      " 'psychology and self-help' 'science' 'Science Fiction & Fantasy'\n",
      " 'romance' 'Teen & YA' 'Politics & Social Sciences' 'thriller' 'Teen & YA'\n",
      " 'thriller' 'romance' 'Politics & Social Sciences' 'history' 'thriller'\n",
      " 'Biographies & Memoirs' 'thriller' 'thriller' 'romance' 'thriller'\n",
      " \"Children's Books\" 'sports' 'romance' 'science' 'thriller' 'thriller'\n",
      " 'thriller' 'thriller' 'thriller' 'Biographies & Memoirs' 'thriller'\n",
      " 'Business & Money' 'romance' 'romance' 'Teen & YA' 'thriller'\n",
      " 'Science Fiction & Fantasy' 'thriller' 'thriller'\n",
      " 'Science Fiction & Fantasy' 'thriller' 'psychology and self-help'\n",
      " \"Children's Books\" 'science' 'crime' 'thriller' 'romance'\n",
      " \"Children's Books\" 'Science Fiction & Fantasy' 'history' 'romance'\n",
      " 'Health' 'romance' 'Science Fiction & Fantasy' 'thriller' 'history'\n",
      " 'Science Fiction & Fantasy' 'thriller' 'Teen & YA' 'thriller' 'romance'\n",
      " 'science' 'thriller' 'Science Fiction & Fantasy' 'thriller' 'thriller'\n",
      " 'thriller' 'history' 'Teen & YA' 'history' \"Children's Books\" 'romance'\n",
      " 'thriller' 'Science Fiction & Fantasy' 'Biographies & Memoirs' 'romance'\n",
      " 'thriller' 'thriller' 'Biographies & Memoirs' 'thriller' 'thriller'\n",
      " 'thriller' 'thriller' 'thriller' 'thriller' 'psychology and self-help'\n",
      " 'romance' 'history' 'thriller' 'Science Fiction & Fantasy' 'history'\n",
      " 'thriller' 'Science Fiction & Fantasy' 'psychology and self-help'\n",
      " 'psychology and self-help' 'history' 'romance' 'Teen & YA' 'history'\n",
      " 'Science Fiction & Fantasy' 'thriller' \"Children's Books\"\n",
      " \"Children's Books\" 'romance' 'romance' 'Biographies & Memoirs' 'thriller'\n",
      " \"Children's Books\" 'travel' 'travel' 'psychology and self-help' 'romance'\n",
      " 'travel' 'travel' 'travel' 'thriller' 'science' 'romance' 'thriller'\n",
      " 'thriller' \"Children's Books\" 'romance' 'travel' \"Children's Books\"\n",
      " 'romance' 'romance' \"Children's Books\" \"Children's Books\" 'romance'\n",
      " 'thriller' 'romance' 'travel' 'Science Fiction & Fantasy'\n",
      " 'Science Fiction & Fantasy' 'history' 'romance'\n",
      " 'Politics & Social Sciences' 'Science Fiction & Fantasy'\n",
      " 'Science Fiction & Fantasy' 'Science Fiction & Fantasy' 'travel'\n",
      " 'Teen & YA' 'thriller' \"Children's Books\" 'Science Fiction & Fantasy'\n",
      " 'thriller' 'Science Fiction & Fantasy' 'thriller'\n",
      " 'Science Fiction & Fantasy' 'thriller' 'travel' 'romance'\n",
      " \"Children's Books\" 'thriller' 'thriller' 'thriller' 'thriller' 'thriller'\n",
      " 'psychology and self-help' \"Children's Books\" 'Science Fiction & Fantasy'\n",
      " 'Biographies & Memoirs' 'Biographies & Memoirs' 'thriller' 'thriller'\n",
      " 'psychology and self-help' 'Biographies & Memoirs' 'crime' 'Health'\n",
      " 'thriller' 'travel' 'thriller' 'thriller' 'thriller'\n",
      " 'psychology and self-help' 'psychology and self-help'\n",
      " 'psychology and self-help' 'psychology and self-help'\n",
      " 'psychology and self-help' 'romance' 'history'\n",
      " 'Science Fiction & Fantasy' 'Science Fiction & Fantasy'\n",
      " 'psychology and self-help' 'thriller' 'thriller' \"Children's Books\"\n",
      " 'Politics & Social Sciences' 'psychology and self-help'\n",
      " 'psychology and self-help' 'Politics & Social Sciences'\n",
      " 'Business & Money' 'psychology and self-help' 'thriller' 'thriller'\n",
      " 'Teen & YA' 'romance' 'Health' 'Crafts, Hobbies & Home'\n",
      " 'Crafts, Hobbies & Home' 'thriller' 'Biographies & Memoirs' 'science'\n",
      " 'travel' 'Biographies & Memoirs' 'thriller' 'thriller' 'thriller'\n",
      " 'Teen & YA' 'Science Fiction & Fantasy' 'thriller' 'Teen & YA' 'thriller'\n",
      " 'thriller' 'thriller' 'crime' 'thriller' 'psychology and self-help'\n",
      " 'romance' \"Children's Books\" 'thriller' 'thriller' 'thriller' 'history'\n",
      " 'psychology and self-help' 'romance' 'thriller'\n",
      " 'Science Fiction & Fantasy' 'romance' 'history' 'thriller' 'thriller'\n",
      " 'Politics & Social Sciences' 'travel' 'thriller' 'history' 'romance'\n",
      " 'history' 'crime' \"Children's Books\" 'history' 'thriller' 'thriller'\n",
      " 'science' 'travel' 'science']\n"
     ]
    }
   ],
   "source": [
    "inv = labelencoder.inverse_transform(genre)\n",
    "print(inv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "mybooks['genre'] = inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>Eva Luna</td>\n",
       "      <td>Children's Books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Pretty Girls</td>\n",
       "      <td>thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Black Mountain</td>\n",
       "      <td>thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>The Unseen (The BarrÃ¸y Chronicles #1)</td>\n",
       "      <td>thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>The Da Vinci Code (Robert Langdon, #2)</td>\n",
       "      <td>thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>The Thursday Murder Club (Thursday Murder Club...</td>\n",
       "      <td>crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Maybe in Another Life</td>\n",
       "      <td>Science Fiction &amp; Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>Why We Sleep: Unlocking the Power of Sleep and...</td>\n",
       "      <td>psychology and self-help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Wahala</td>\n",
       "      <td>romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>The Heart's Invisible Furies</td>\n",
       "      <td>thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>Never Let Me Go</td>\n",
       "      <td>thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>The Firm (The Firm, #1)</td>\n",
       "      <td>thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>A Broken Blade (The Halfling Saga, #1)</td>\n",
       "      <td>Science Fiction &amp; Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>A Little Life</td>\n",
       "      <td>thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>Leave the World Behind</td>\n",
       "      <td>thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>The Quiet Tenant</td>\n",
       "      <td>thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>Mrs. Dalloway</td>\n",
       "      <td>thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>How to Kill Your Family</td>\n",
       "      <td>thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>The Glass Hotel</td>\n",
       "      <td>thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>We Were Liars</td>\n",
       "      <td>thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>Concepts of Pattern Grading: Techniques for Ma...</td>\n",
       "      <td>Health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>Violeta</td>\n",
       "      <td>Children's Books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>A Long Petal of the Sea</td>\n",
       "      <td>thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>Tomb of Sand</td>\n",
       "      <td>thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Penance</td>\n",
       "      <td>Biographies &amp; Memoirs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>The Gangster We Are All Looking For</td>\n",
       "      <td>Children's Books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>Anxious People</td>\n",
       "      <td>Children's Books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Fahrenheit 451</td>\n",
       "      <td>Science Fiction &amp; Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>The Accidental Alchemist (An Accidental Alchem...</td>\n",
       "      <td>Science Fiction &amp; Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>Patternmaking for Fashion Design</td>\n",
       "      <td>Crafts, Hobbies &amp; Home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>Memoirs of a Geisha</td>\n",
       "      <td>Biographies &amp; Memoirs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>In the Time of the Butterflies</td>\n",
       "      <td>history</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>CafÃ© Nevo: A Novel</td>\n",
       "      <td>Teen &amp; YA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>The Coworker</td>\n",
       "      <td>thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Sweet Bean Paste</td>\n",
       "      <td>romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>The Book Thief</td>\n",
       "      <td>Science Fiction &amp; Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>One Hundred Years of Solitude</td>\n",
       "      <td>history</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>Normal People</td>\n",
       "      <td>thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>The Ballad of Songbirds and Snakes (The Hunger...</td>\n",
       "      <td>history</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>The Prison Healer (The Prison Healer, #1)</td>\n",
       "      <td>Teen &amp; YA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The House in the Cerulean Sea</td>\n",
       "      <td>Science Fiction &amp; Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Icebreaker (Maple Hills, #1)</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Tomorrow, and Tomorrow, and Tomorrow</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>An American Marriage</td>\n",
       "      <td>Biographies &amp; Memoirs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>Siege and Storm (Grisha Verse, #2)</td>\n",
       "      <td>Teen &amp; YA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>Neverwhere (London Below, #1)</td>\n",
       "      <td>Children's Books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>Breasts and Eggs</td>\n",
       "      <td>romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>None of This Is True</td>\n",
       "      <td>thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>Professional Pattern Grading for Women'S, Men'...</td>\n",
       "      <td>Crafts, Hobbies &amp; Home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>A Court of Thorns and Roses (A Court of Thorns...</td>\n",
       "      <td>Science Fiction &amp; Fantasy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Title  \\\n",
       "202                                           Eva Luna   \n",
       "77                                        Pretty Girls   \n",
       "88                                      Black Mountain   \n",
       "298              The Unseen (The BarrÃ¸y Chronicles #1)   \n",
       "60              The Da Vinci Code (Robert Langdon, #2)   \n",
       "280  The Thursday Murder Club (Thursday Murder Club...   \n",
       "42                               Maybe in Another Life   \n",
       "292  Why We Sleep: Unlocking the Power of Sleep and...   \n",
       "91                                              Wahala   \n",
       "287                       The Heart's Invisible Furies   \n",
       "123                                    Never Let Me Go   \n",
       "59                             The Firm (The Firm, #1)   \n",
       "35              A Broken Blade (The Halfling Saga, #1)   \n",
       "308                                      A Little Life   \n",
       "299                             Leave the World Behind   \n",
       "23                                    The Quiet Tenant   \n",
       "255                                      Mrs. Dalloway   \n",
       "257                            How to Kill Your Family   \n",
       "277                                    The Glass Hotel   \n",
       "334                                      We Were Liars   \n",
       "311  Concepts of Pattern Grading: Techniques for Ma...   \n",
       "226                                            Violeta   \n",
       "328                            A Long Petal of the Sea   \n",
       "181                                       Tomb of Sand   \n",
       "53                                             Penance   \n",
       "357                The Gangster We Are All Looking For   \n",
       "142                                     Anxious People   \n",
       "13                                      Fahrenheit 451   \n",
       "252  The Accidental Alchemist (An Accidental Alchem...   \n",
       "313                   Patternmaking for Fashion Design   \n",
       "275                                Memoirs of a Geisha   \n",
       "353                     In the Time of the Butterflies   \n",
       "133                                 CafÃ© Nevo: A Novel   \n",
       "12                                        The Coworker   \n",
       "100                                   Sweet Bean Paste   \n",
       "273                                     The Book Thief   \n",
       "149                      One Hundred Years of Solitude   \n",
       "220                                      Normal People   \n",
       "87   The Ballad of Songbirds and Snakes (The Hunger...   \n",
       "30           The Prison Healer (The Prison Healer, #1)   \n",
       "2                        The House in the Cerulean Sea   \n",
       "89                        Icebreaker (Maple Hills, #1)   \n",
       "94                Tomorrow, and Tomorrow, and Tomorrow   \n",
       "174                               An American Marriage   \n",
       "324                 Siege and Storm (Grisha Verse, #2)   \n",
       "203                      Neverwhere (London Below, #1)   \n",
       "106                                   Breasts and Eggs   \n",
       "26                                None of This Is True   \n",
       "312  Professional Pattern Grading for Women'S, Men'...   \n",
       "135  A Court of Thorns and Roses (A Court of Thorns...   \n",
       "\n",
       "                         genre  \n",
       "202           Children's Books  \n",
       "77                    thriller  \n",
       "88                    thriller  \n",
       "298                   thriller  \n",
       "60                    thriller  \n",
       "280                      crime  \n",
       "42   Science Fiction & Fantasy  \n",
       "292   psychology and self-help  \n",
       "91                     romance  \n",
       "287                   thriller  \n",
       "123                   thriller  \n",
       "59                    thriller  \n",
       "35   Science Fiction & Fantasy  \n",
       "308                   thriller  \n",
       "299                   thriller  \n",
       "23                    thriller  \n",
       "255                   thriller  \n",
       "257                   thriller  \n",
       "277                   thriller  \n",
       "334                   thriller  \n",
       "311                     Health  \n",
       "226           Children's Books  \n",
       "328                   thriller  \n",
       "181                   thriller  \n",
       "53       Biographies & Memoirs  \n",
       "357           Children's Books  \n",
       "142           Children's Books  \n",
       "13   Science Fiction & Fantasy  \n",
       "252  Science Fiction & Fantasy  \n",
       "313     Crafts, Hobbies & Home  \n",
       "275      Biographies & Memoirs  \n",
       "353                    history  \n",
       "133                  Teen & YA  \n",
       "12                    thriller  \n",
       "100                    romance  \n",
       "273  Science Fiction & Fantasy  \n",
       "149                    history  \n",
       "220                   thriller  \n",
       "87                     history  \n",
       "30                   Teen & YA  \n",
       "2    Science Fiction & Fantasy  \n",
       "89                      sports  \n",
       "94                      sports  \n",
       "174      Biographies & Memoirs  \n",
       "324                  Teen & YA  \n",
       "203           Children's Books  \n",
       "106                    romance  \n",
       "26                    thriller  \n",
       "312     Crafts, Hobbies & Home  \n",
       "135  Science Fiction & Fantasy  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mybooks[['Title','genre']].sample(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "genre\n",
       "thriller                      116\n",
       "romance                        47\n",
       "Science Fiction & Fantasy      32\n",
       "psychology and self-help       23\n",
       "history                        19\n",
       "Children's Books               18\n",
       "travel                         17\n",
       "Teen & YA                      13\n",
       "Biographies & Memoirs          11\n",
       "science                         9\n",
       "Politics & Social Sciences      6\n",
       "crime                           5\n",
       "Business & Money                5\n",
       "sports                          3\n",
       "Health                          3\n",
       "horror                          2\n",
       "Crafts, Hobbies & Home          2\n",
       "Cookbooks, Food & Wine          1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mybooks.genre.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating prediction of model \n",
    "\n",
    "It seems like a lot of books are not predicted corretly and a majority of my books are predicted as thriller. I do read a lot of thrillers but the share is too big too be true. It does seem like the initival dataset used for prediction have a substantial share of thrillers, which may cause it too predict too many books as thrillers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving the model \n",
    "I will try and add another dataset for training, https://github.com/uchidalab/book-dataset/tree/master/Task2, which contains 270K books from amazon with title and category name. \n",
    "\n",
    "I will have to: \n",
    "- make sure common category names between two datasets are the same \n",
    "- remove some small and unecessary categories from the amazon dataframe. \n",
    "- collect description from google api in several rounds with max 49K books each time to now exceed limit of 50K per day. \n",
    "\n",
    "Model building: \n",
    "- use similar approach and test several models: https://www.kaggle.com/code/prathameshgadekar/book-genre-prediction-nlp/notebook\n",
    "- select best predicting model "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Data cleaning - run once\n",
    "amazondf = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/uchidalab/book-dataset/master/Task2/book32-listing.csv\", \n",
    "    encoding='ISO-8859-1',\n",
    "    header=None, \n",
    "    names=[\"AMAZON INDEX (ASIN)}\",\"[FILENAME]\",\"[IMAGE URL]\",\"title\",\"author\",\"[CATEGORY ID]\",\"category\"])\n",
    "amazondf = amazondf[['title', 'author', 'category']] \n",
    "\n",
    "# I remove some categories I dont feel will be books added to GR (usually fiction) and some very small categories. \n",
    "\n",
    "new_labels = {\"Travel\"        : \"travel\",\n",
    "\"Children's Books\"            : \"Children's Books\"  ,\n",
    "\"Health, Fitness & Dieting\"   : \"Health\",\n",
    "\"Business & Money\"            : \"Business & Money\"  ,\n",
    "\"Crafts, Hobbies & Home\"      : \"Crafts, Hobbies & Home\" ,\n",
    "\"Cookbooks, Food & Wine\"      : \"Cookbooks, Food & Wine\"  ,\n",
    "\"Teen & Young Adult\"          : \"Teen & YA\",\n",
    "\"History\"                     : \"history\",\n",
    "\"Sports & Outdoors\"           : \"sports\",\n",
    "\"Romance\"                     : \"romance\",\n",
    "\"Biographies & Memoirs\"       : \"Biographies & Memoirs\",\n",
    "\"Science Fiction & Fantasy\"   : \"Science Fiction & Fantasy\",\n",
    "\"Politics & Social Sciences\"  : \"Politics & Social Sciences\",\n",
    "\"Self-Help\"                   : 'psychology and self-help' ,\n",
    "\"Mystery, Thriller & Suspense\": \"thriller\"}\n",
    "\n",
    "\n",
    "amazondf = amazondf.query('category in @new_labels')\n",
    "amazondf['category'] = amazondf['category'].replace(new_labels)\n",
    "\n",
    "amazondf = amazondf.rename(columns={'category':'genre', 'title':'Title', 'author':'Author'} )\n",
    "amazondf = amazondf.drop_duplicates(subset='Title', keep='first')\n",
    "\n",
    "amazondf.to_pickle('assets/amazon_books.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
